{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/macul/libraries/mk_utils/mklib/nn/tfnet/')\n",
    "sys.path.append('/home/macul/libraries/mk_utils/mklib/nn/tfloss/')\n",
    "sys.path.append('/home/macul/libraries/mk_utils/tf_spoof/')\n",
    "sys.path.append('/home/macul/libraries/mk_utils/spoofing_lbp/')\n",
    "from config import net_config as config\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from os.path import isdir\n",
    "from os import mkdir\n",
    "from shutil import copyfile\n",
    "from spoof_feature import LBP_GREY, LBP_RG, CoALBP, CoALBP_256, CoALBP_GREY, BSIF, LPQ, COLOR_MOMENT\n",
    "from tfspoofdense import SpoofDenseNet\n",
    "from tfloss import TfLosses\n",
    "import time\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "Model_Process = {'BSIF'        : BSIF(),\n",
    "                 'CoALBP'      : CoALBP(),\n",
    "                 'CoALBP_GREY' : CoALBP_GREY(),\n",
    "                 'CoALBP_256'  : CoALBP_256(),\n",
    "                 'COLOR_MOMENT': COLOR_MOMENT(),\n",
    "                 'LBP_GREY'    : LBP_GREY(),\n",
    "                 'LBP_RG'      : LBP_RG(),\n",
    "                 'LPQ'         : LPQ(),\n",
    "                }\n",
    "\n",
    "# get ft extractor and network configuration\n",
    "ft_extractor = [Model_Process[ex] for ex in config.Feature_Type]\n",
    "ft_layer_units = [config.Dense_Cfg[ex] for ex in config.Feature_Type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.DEVICE_IDS\n",
    "#batchSize = config.BATCH_SIZE * config.NUM_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop_shuffle(img, height, width, grid_size=4):\n",
    "    crop_h = tf.cast(height/4,tf.int32)\n",
    "    crop_w = tf.cast(width/4,tf.int32)\n",
    "    \n",
    "    idx = list(range(grid_size*grid_size))\n",
    "    np.random.shuffle(idx)\n",
    "    img_tmp = []\n",
    "    cnt = 0\n",
    "    for i, v in enumerate(idx):\n",
    "        if i%grid_size == 0:\n",
    "            img_crop = tf.image.crop_to_bounding_box(img, int(idx[i]/4)*crop_h,int(idx[i]%4)*crop_w,crop_h,crop_w)\n",
    "        else:\n",
    "            img_crop = tf.concat([img_crop, tf.image.crop_to_bounding_box(img, int(idx[i]/4)*crop_h,int(idx[i]%4)*crop_w,crop_h,crop_w)], 1)\n",
    "        cnt += 1\n",
    "        if cnt == grid_size:\n",
    "            img_tmp += [img_crop]\n",
    "            cnt = 0\n",
    "    img1 = img_tmp[0]\n",
    "    for i in range(1,len(img_tmp)):\n",
    "        img1 = tf.concat([img1,img_tmp[i]],0)\n",
    "        \n",
    "    return img1\n",
    "\n",
    "\n",
    "# this function must be used for batch_size of 1 or before batch operation since the image size varies\n",
    "def parse_function(example_proto, grid_size=4):\n",
    "    \n",
    "    features = {'image/encoded': tf.FixedLenFeature([], tf.string),\n",
    "                'image/height': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/width': tf.FixedLenFeature([], tf.int64),\n",
    "                #'image/colorspace': tf.FixedLenFeature([], tf.string),\n",
    "                #'image/channels': tf.FixedLenFeature([], tf.int64),\n",
    "                #'image/class/text': tf.FixedLenFeature([], tf.string),\n",
    "                'image/class/label': tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    img = tf.image.decode_jpeg(features['image/encoded'])\n",
    "    #img = features['image/encoded']\n",
    "    \n",
    "    # img = tf.reshape(img, shape=(112, 112, 3))\n",
    "    # r, g, b = tf.split(img, num_or_size_splits=3, axis=-1)\n",
    "    # img = tf.concat([b, g, r], axis=-1)\n",
    "    \n",
    "    #img = tf.cast(img, dtype=tf.float32)\n",
    "    #img = tf.subtract(img, 127.5)\n",
    "    #img = tf.multiply(img,  0.0078125)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    \n",
    "    label = tf.cast(features['image/class/label'], tf.int64) - 1\n",
    "    \n",
    "    if grid_size <= 1:\n",
    "        return img, label\n",
    "    \n",
    "    height = tf.cast(features['image/height'], tf.int64)\n",
    "    width = tf.cast(features['image/width'], tf.int64)\n",
    "    \n",
    "    return img_crop_shuffle(img,height,width,grid_size), label\n",
    "\n",
    "def feature_extraction(img, extractor):\n",
    "    ft = []\n",
    "    for ex in extractor:\n",
    "        ft += [tf.cast(tf.py_func(ex, [img], tf.double),tf.float32)]\n",
    "    return tuple(ft) # list does not work here, need to convert to tuple\n",
    "    #return tuple(ft), img # for dataset debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# dataset testing\n",
    "dataset_train = tf.data.TFRecordDataset(config.TRAIN_REC)\n",
    "\n",
    "#for raw_record in dataset_train.take(1):\n",
    "#    print(repr(raw_record))\n",
    "\n",
    "dataset_train = dataset_train.map(lambda x: parse_function(x, config.Img_shuffle_grid_size))\n",
    "#dataset_train = dataset_train.shuffle(buffer_size=config.BUFFER_SIZE) # shuffle the whole dataset is better\n",
    "#dataset_train = dataset_train.batch(1)\n",
    "#dataset_train = dataset_train.apply(tf.data.experimental.unbatch())\n",
    "dataset_train = dataset_train.map(lambda *x: (feature_extraction(x[0], ft_extractor), x[1]))\n",
    "dataset_train = dataset_train.batch(4)\n",
    "iterator_train = dataset_train.make_initializable_iterator()\n",
    "next_element_train = iterator_train.get_next()\n",
    "\n",
    "cfg = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "cfg.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=cfg)\n",
    "sess.run(iterator_train.initializer)\n",
    "\n",
    "feature, label = sess.run(next_element_train)\n",
    "'''\n",
    "\n",
    "#training_ph = tf.placeholder(name='training_flag',shape=[], dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_scope = 'SpoofDenseNet'\n",
    "\n",
    "# declare placeholders\n",
    "labels_ph = tf.placeholder(name='label',shape=[None,], dtype=tf.int64)\n",
    "features_ph = tuple([tf.placeholder(name='ft_{}'.format(f_key),shape=[None,config.Feature_Size[f_key]], \n",
    "                                    dtype=tf.float32) for f_key in config.Feature_Type])\n",
    "# build training dataset and network\n",
    "dataset_train = tf.data.TFRecordDataset(config.TRAIN_REC)\n",
    "dataset_train = dataset_train.map(lambda x: parse_function(x, config.Img_shuffle_grid_size))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=config.BUFFER_SIZE) # shuffle the whole dataset is better\n",
    "dataset_train = dataset_train.map(lambda *x: (feature_extraction(x[0], ft_extractor), x[1]))\n",
    "# remember to set batch size to 1 for dataset debug\n",
    "#dataset_train = dataset_train.map(lambda *x: feature_extraction(x[0], ft_extractor) + (x[1],)) # for dataset debug\n",
    "dataset_train = dataset_train.batch(config.BATCH_SIZE)\n",
    "iterator_train = dataset_train.make_initializable_iterator()\n",
    "next_element_train = iterator_train.get_next()\n",
    "with tf.variable_scope(net_scope):\n",
    "    # build base network\n",
    "    embeddings = SpoofDenseNet.build(features_ph, feature_units=ft_layer_units, stem_units=config.Dense_Stem_Cfg, \n",
    "                                     training=True, act=config.Activation, reg=config.Regularizer, \n",
    "                                     init=config.Initializer, scope='spoof')\n",
    "    #logit, inference_loss = TfLosses.softmax_loss(embedding=embeddings, labels=labels_ph, out_num=config.NUM_CLASSES,  \n",
    "    #                                              act=config.Activation, reg=config.Regularizer, \n",
    "    #                                              init=config.Initializer)\n",
    "    logit, inference_loss = TfLosses.arc_loss(embedding=embeddings, labels=labels_ph, w_init=config.Initializer, \n",
    "                                              out_num=config.NUM_CLASSES, s=config.Arc_margin_scale, \n",
    "                                              m=config.Arc_margin_angle)\n",
    "    pred = tf.nn.softmax(logit)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, axis=1), labels_ph), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for training\n",
    "global_step = tf.Variable(name='global_step', initial_value=0, trainable=False)\n",
    "inc_op = tf.assign_add(global_step, 1, name='increment_global_step')\n",
    "lr = tf.train.piecewise_constant(global_step, boundaries=config.Opt_lr_steps, values=config.Opt_lr, name='lr_schedule')\n",
    "# define the optimize method\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=config.Opt_momentum)\n",
    "# get train op\n",
    "#grads = opt.compute_gradients(inference_loss)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):        \n",
    "    #train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    train_op = opt.minimize(inference_loss, global_step=global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare placeholders\n",
    "labels_val_ph = tf.placeholder(name='label',shape=[None,], dtype=tf.int64)\n",
    "features_val_ph = tuple([tf.placeholder(name='ft_{}'.format(f_key),shape=[None,config.Feature_Size[f_key]], \n",
    "                                    dtype=tf.float32) for f_key in config.Feature_Type])\n",
    "# build validation dataset and network\n",
    "dataset_val = tf.data.TFRecordDataset(config.VAL_REC)\n",
    "dataset_val = dataset_val.map(lambda x: parse_function(x, 0))\n",
    "dataset_val = dataset_val.map(lambda *x: (feature_extraction(x[0], ft_extractor), x[1]))\n",
    "dataset_val = dataset_val.batch(config.BATCH_SIZE)\n",
    "iterator_val = dataset_val.make_initializable_iterator()\n",
    "next_element_val = iterator_val.get_next()\n",
    "with tf.variable_scope(net_scope, reuse=True):\n",
    "    # build base network\n",
    "    embeddings_val = SpoofDenseNet.build(features_val_ph, feature_units=ft_layer_units, stem_units=config.Dense_Stem_Cfg, \n",
    "                                         training=False, act=config.Activation, reg=config.Regularizer, \n",
    "                                         init=config.Initializer, scope='spoof')\n",
    "    logit_val, _ = TfLosses.arc_loss(embedding=embeddings_val, labels=labels_val_ph, w_init=config.Initializer, \n",
    "                                     out_num=config.NUM_CLASSES, s=config.Arc_margin_scale, m=0)\n",
    "    pred_val = tf.nn.softmax(logit_val)\n",
    "    acc_val = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred_val, axis=1), labels_val_ph), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create session\n",
    "cfg = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "cfg.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=cfg)\n",
    "# create model saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "time_stamp = time.strftime('_%Y-%m-%d-%H-%M', time.localtime(time.time()))\n",
    "\n",
    "# create output folder\n",
    "checkpointsPath = os.path.sep.join([config.OUT_PATH, config.PREFIX])\n",
    "if not isdir(checkpointsPath):\n",
    "    mkdir(checkpointsPath)\n",
    "    \n",
    "# define log file\n",
    "log_file_path = checkpointsPath + '/train' + time_stamp + '.log'\n",
    "log_file = open(log_file_path, 'w')\n",
    "\n",
    "# summary writer\n",
    "summary = tf.summary.FileWriter(checkpointsPath, sess.graph)\n",
    "summaries = []\n",
    "# trainabel variable gradients\n",
    "for var in tf.trainable_variables():\n",
    "    summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "# add loss summary\n",
    "summaries.append(tf.summary.scalar('inference_loss', inference_loss))\n",
    "# add learning rate\n",
    "summaries.append(tf.summary.scalar('leraning_rate', lr))\n",
    "# add accuracy\n",
    "summaries.append(tf.summary.scalar('accuracy', acc))\n",
    "summary_op = tf.summary.merge(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 0, total_step 10, inference loss is 4.27, accuracy is 0.570312, time 302.997 samples/sec\n",
      "Training: epoch 0, total_step 20, inference loss is 3.29, accuracy is 0.656250, time 266.000 samples/sec\n",
      "Training: epoch 0, total_step 30, inference loss is 2.56, accuracy is 0.718750, time 224.634 samples/sec\n",
      "Training: epoch 0, total_step 40, inference loss is 2.20, accuracy is 0.710938, time 237.001 samples/sec\n",
      "Training: epoch 0, total_step 50, inference loss is 1.76, accuracy is 0.726562, time 246.808 samples/sec\n",
      "Training: epoch 0, total_step 60, inference loss is 1.67, accuracy is 0.695312, time 293.918 samples/sec\n",
      "Training: epoch 0, total_step 70, inference loss is 2.60, accuracy is 0.710938, time 237.063 samples/sec\n",
      "Training: epoch 0, total_step 80, inference loss is 1.31, accuracy is 0.710938, time 215.227 samples/sec\n",
      "Training: epoch 0, total_step 90, inference loss is 1.09, accuracy is 0.789062, time 256.535 samples/sec\n",
      "Training: epoch 0, total_step 100, inference loss is 1.50, accuracy is 0.734375, time 212.713 samples/sec\n",
      "Training: epoch 0, total_step 110, inference loss is 1.09, accuracy is 0.781250, time 222.897 samples/sec\n",
      "Training: epoch 0, total_step 120, inference loss is 1.01, accuracy is 0.742188, time 248.370 samples/sec\n",
      "Training: epoch 0, total_step 130, inference loss is 0.71, accuracy is 0.796875, time 235.221 samples/sec\n",
      "Training: epoch 0, total_step 140, inference loss is 0.71, accuracy is 0.796875, time 241.998 samples/sec\n",
      "Training: epoch 0, total_step 150, inference loss is 0.66, accuracy is 0.765625, time 243.646 samples/sec\n",
      "Training: epoch 0, total_step 160, inference loss is 0.85, accuracy is 0.742188, time 298.044 samples/sec\n",
      "Training: epoch 0, total_step 170, inference loss is 0.58, accuracy is 0.773438, time 277.049 samples/sec\n",
      "Training: epoch 0, total_step 180, inference loss is 0.63, accuracy is 0.773438, time 237.916 samples/sec\n",
      "Training: epoch 0, total_step 190, inference loss is 0.57, accuracy is 0.789062, time 246.852 samples/sec\n",
      "Training: epoch 0, total_step 200, inference loss is 0.72, accuracy is 0.726562, time 248.488 samples/sec\n",
      "Training: epoch 0, total_step 210, inference loss is 0.53, accuracy is 0.750000, time 260.331 samples/sec\n",
      "Training: epoch 0, total_step 220, inference loss is 0.49, accuracy is 0.843750, time 243.752 samples/sec\n",
      "Training: epoch 0, total_step 230, inference loss is 0.40, accuracy is 0.828125, time 230.696 samples/sec\n",
      "Training: epoch 0, total_step 240, inference loss is 0.75, accuracy is 0.757812, time 253.857 samples/sec\n",
      "Training: epoch 0, total_step 250, inference loss is 0.64, accuracy is 0.773438, time 282.940 samples/sec\n",
      "Training: epoch 0, total_step 260, inference loss is 0.63, accuracy is 0.781250, time 316.618 samples/sec\n",
      "Training: epoch 0, total_step 270, inference loss is 0.57, accuracy is 0.835938, time 256.367 samples/sec\n",
      "Training: epoch 0, total_step 280, inference loss is 0.62, accuracy is 0.789062, time 235.918 samples/sec\n",
      "Training: epoch 0, total_step 290, inference loss is 0.55, accuracy is 0.781250, time 202.492 samples/sec\n",
      "Training: epoch 0, total_step 300, inference loss is 0.56, accuracy is 0.781250, time 275.165 samples/sec\n",
      "Training: epoch 0, total_step 310, inference loss is 0.50, accuracy is 0.820312, time 232.110 samples/sec\n",
      "Training: epoch 0, total_step 320, inference loss is 0.55, accuracy is 0.750000, time 211.348 samples/sec\n",
      "Training: epoch 0, total_step 330, inference loss is 0.39, accuracy is 0.843750, time 256.935 samples/sec\n",
      "Training: epoch 0, total_step 340, inference loss is 0.44, accuracy is 0.820312, time 207.381 samples/sec\n",
      "Training: epoch 0, total_step 350, inference loss is 0.61, accuracy is 0.789062, time 239.132 samples/sec\n",
      "Training: epoch 0, total_step 360, inference loss is 0.50, accuracy is 0.804688, time 251.293 samples/sec\n",
      "Training: epoch 0, total_step 370, inference loss is 0.39, accuracy is 0.851562, time 284.259 samples/sec\n",
      "Training: epoch 0, total_step 380, inference loss is 0.63, accuracy is 0.796875, time 249.156 samples/sec\n",
      "End of epoch 0\n",
      "$$$$$$$$ Validation: epoch 0, accuracy is 0.876310\n",
      "Training: epoch 1, total_step 390, inference loss is 0.37, accuracy is 0.851562, time 314.872 samples/sec\n",
      "Training: epoch 1, total_step 400, inference loss is 0.36, accuracy is 0.820312, time 233.709 samples/sec\n",
      "Training: epoch 1, total_step 410, inference loss is 0.49, accuracy is 0.851562, time 209.409 samples/sec\n",
      "Training: epoch 1, total_step 420, inference loss is 0.45, accuracy is 0.828125, time 196.555 samples/sec\n",
      "Training: epoch 1, total_step 430, inference loss is 0.39, accuracy is 0.867188, time 277.300 samples/sec\n",
      "Training: epoch 1, total_step 440, inference loss is 0.53, accuracy is 0.781250, time 220.556 samples/sec\n",
      "Training: epoch 1, total_step 450, inference loss is 0.42, accuracy is 0.851562, time 226.780 samples/sec\n",
      "Training: epoch 1, total_step 460, inference loss is 0.52, accuracy is 0.820312, time 293.912 samples/sec\n",
      "Training: epoch 1, total_step 470, inference loss is 0.45, accuracy is 0.835938, time 291.989 samples/sec\n",
      "Training: epoch 1, total_step 480, inference loss is 0.43, accuracy is 0.828125, time 223.839 samples/sec\n",
      "Training: epoch 1, total_step 490, inference loss is 0.43, accuracy is 0.789062, time 285.210 samples/sec\n",
      "Training: epoch 1, total_step 500, inference loss is 0.42, accuracy is 0.828125, time 220.870 samples/sec\n",
      "Training: epoch 1, total_step 510, inference loss is 0.42, accuracy is 0.875000, time 219.576 samples/sec\n",
      "Training: epoch 1, total_step 520, inference loss is 0.36, accuracy is 0.859375, time 233.844 samples/sec\n",
      "Training: epoch 1, total_step 530, inference loss is 0.45, accuracy is 0.828125, time 270.836 samples/sec\n",
      "Training: epoch 1, total_step 540, inference loss is 0.39, accuracy is 0.843750, time 175.926 samples/sec\n",
      "Training: epoch 1, total_step 550, inference loss is 0.42, accuracy is 0.843750, time 232.762 samples/sec\n",
      "Training: epoch 1, total_step 560, inference loss is 0.33, accuracy is 0.890625, time 279.334 samples/sec\n",
      "Training: epoch 1, total_step 570, inference loss is 0.43, accuracy is 0.859375, time 286.878 samples/sec\n",
      "Training: epoch 1, total_step 580, inference loss is 0.51, accuracy is 0.820312, time 212.958 samples/sec\n",
      "Training: epoch 1, total_step 590, inference loss is 0.47, accuracy is 0.796875, time 222.508 samples/sec\n",
      "Training: epoch 1, total_step 600, inference loss is 0.42, accuracy is 0.859375, time 254.459 samples/sec\n",
      "Training: epoch 1, total_step 610, inference loss is 0.38, accuracy is 0.882812, time 275.341 samples/sec\n",
      "Training: epoch 1, total_step 620, inference loss is 0.38, accuracy is 0.859375, time 234.652 samples/sec\n",
      "Training: epoch 1, total_step 630, inference loss is 0.28, accuracy is 0.890625, time 269.658 samples/sec\n",
      "Training: epoch 1, total_step 640, inference loss is 0.44, accuracy is 0.796875, time 291.983 samples/sec\n",
      "Training: epoch 1, total_step 650, inference loss is 0.38, accuracy is 0.867188, time 216.018 samples/sec\n",
      "Training: epoch 1, total_step 660, inference loss is 0.51, accuracy is 0.773438, time 247.721 samples/sec\n",
      "Training: epoch 1, total_step 670, inference loss is 0.42, accuracy is 0.851562, time 230.035 samples/sec\n",
      "Training: epoch 1, total_step 680, inference loss is 0.32, accuracy is 0.882812, time 243.919 samples/sec\n",
      "Training: epoch 1, total_step 690, inference loss is 0.39, accuracy is 0.875000, time 200.411 samples/sec\n",
      "Training: epoch 1, total_step 700, inference loss is 0.36, accuracy is 0.851562, time 297.849 samples/sec\n",
      "Training: epoch 1, total_step 710, inference loss is 0.38, accuracy is 0.859375, time 211.327 samples/sec\n",
      "Training: epoch 1, total_step 720, inference loss is 0.40, accuracy is 0.867188, time 205.746 samples/sec\n",
      "Training: epoch 1, total_step 730, inference loss is 0.37, accuracy is 0.890625, time 185.710 samples/sec\n",
      "Training: epoch 1, total_step 740, inference loss is 0.40, accuracy is 0.859375, time 234.247 samples/sec\n",
      "Training: epoch 1, total_step 750, inference loss is 0.33, accuracy is 0.851562, time 238.593 samples/sec\n",
      "Training: epoch 1, total_step 760, inference loss is 0.33, accuracy is 0.882812, time 217.478 samples/sec\n",
      "Training: epoch 1, total_step 770, inference loss is 0.33, accuracy is 0.914062, time 267.322 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1\n",
      "$$$$$$$$ Validation: epoch 1, accuracy is 0.918042\n",
      "Training: epoch 2, total_step 780, inference loss is 0.43, accuracy is 0.820312, time 194.468 samples/sec\n",
      "Training: epoch 2, total_step 790, inference loss is 0.45, accuracy is 0.835938, time 207.608 samples/sec\n",
      "Training: epoch 2, total_step 800, inference loss is 0.43, accuracy is 0.859375, time 217.132 samples/sec\n",
      "Training: epoch 2, total_step 810, inference loss is 0.33, accuracy is 0.875000, time 258.141 samples/sec\n",
      "Training: epoch 2, total_step 820, inference loss is 0.51, accuracy is 0.812500, time 195.565 samples/sec\n",
      "Training: epoch 2, total_step 830, inference loss is 0.39, accuracy is 0.875000, time 272.401 samples/sec\n",
      "Training: epoch 2, total_step 840, inference loss is 0.42, accuracy is 0.835938, time 217.955 samples/sec\n",
      "Training: epoch 2, total_step 850, inference loss is 0.48, accuracy is 0.820312, time 229.106 samples/sec\n",
      "Training: epoch 2, total_step 860, inference loss is 0.37, accuracy is 0.867188, time 268.015 samples/sec\n",
      "Training: epoch 2, total_step 870, inference loss is 0.39, accuracy is 0.843750, time 267.021 samples/sec\n",
      "Training: epoch 2, total_step 880, inference loss is 0.44, accuracy is 0.828125, time 216.979 samples/sec\n",
      "Training: epoch 2, total_step 890, inference loss is 0.49, accuracy is 0.789062, time 236.524 samples/sec\n",
      "Training: epoch 2, total_step 900, inference loss is 0.45, accuracy is 0.851562, time 215.765 samples/sec\n",
      "Training: epoch 2, total_step 910, inference loss is 0.53, accuracy is 0.781250, time 257.032 samples/sec\n",
      "Training: epoch 2, total_step 920, inference loss is 0.37, accuracy is 0.843750, time 208.705 samples/sec\n",
      "Training: epoch 2, total_step 930, inference loss is 0.37, accuracy is 0.867188, time 211.176 samples/sec\n",
      "Training: epoch 2, total_step 940, inference loss is 0.65, accuracy is 0.812500, time 175.052 samples/sec\n",
      "Training: epoch 2, total_step 950, inference loss is 0.37, accuracy is 0.835938, time 243.758 samples/sec\n",
      "Training: epoch 2, total_step 960, inference loss is 0.48, accuracy is 0.820312, time 236.305 samples/sec\n",
      "Training: epoch 2, total_step 970, inference loss is 0.48, accuracy is 0.812500, time 234.142 samples/sec\n",
      "Training: epoch 2, total_step 980, inference loss is 0.37, accuracy is 0.875000, time 242.103 samples/sec\n",
      "Training: epoch 2, total_step 990, inference loss is 0.30, accuracy is 0.906250, time 231.036 samples/sec\n",
      "Training: epoch 2, total_step 1000, inference loss is 0.27, accuracy is 0.898438, time 262.294 samples/sec\n",
      "Training: epoch 2, total_step 1010, inference loss is 0.36, accuracy is 0.875000, time 157.533 samples/sec\n",
      "Training: epoch 2, total_step 1020, inference loss is 0.40, accuracy is 0.867188, time 220.547 samples/sec\n",
      "Training: epoch 2, total_step 1030, inference loss is 0.44, accuracy is 0.851562, time 257.052 samples/sec\n",
      "Training: epoch 2, total_step 1040, inference loss is 0.55, accuracy is 0.796875, time 194.130 samples/sec\n",
      "Training: epoch 2, total_step 1050, inference loss is 0.40, accuracy is 0.882812, time 282.646 samples/sec\n",
      "Training: epoch 2, total_step 1060, inference loss is 0.33, accuracy is 0.859375, time 239.820 samples/sec\n",
      "Training: epoch 2, total_step 1070, inference loss is 0.29, accuracy is 0.898438, time 241.068 samples/sec\n",
      "Training: epoch 2, total_step 1080, inference loss is 0.36, accuracy is 0.875000, time 266.463 samples/sec\n",
      "Training: epoch 2, total_step 1090, inference loss is 0.34, accuracy is 0.867188, time 254.964 samples/sec\n",
      "Training: epoch 2, total_step 1100, inference loss is 0.51, accuracy is 0.828125, time 239.362 samples/sec\n",
      "Training: epoch 2, total_step 1110, inference loss is 0.43, accuracy is 0.859375, time 249.091 samples/sec\n",
      "Training: epoch 2, total_step 1120, inference loss is 0.28, accuracy is 0.882812, time 210.985 samples/sec\n",
      "Training: epoch 2, total_step 1130, inference loss is 0.31, accuracy is 0.882812, time 265.936 samples/sec\n",
      "Training: epoch 2, total_step 1140, inference loss is 0.40, accuracy is 0.843750, time 198.837 samples/sec\n",
      "Training: epoch 2, total_step 1150, inference loss is 0.33, accuracy is 0.875000, time 216.310 samples/sec\n",
      "Training: epoch 2, total_step 1160, inference loss is 0.34, accuracy is 0.882812, time 270.987 samples/sec\n",
      "End of epoch 2\n",
      "$$$$$$$$ Validation: epoch 2, accuracy is 0.902148\n",
      "Training: epoch 3, total_step 1170, inference loss is 0.44, accuracy is 0.882812, time 229.910 samples/sec\n",
      "Training: epoch 3, total_step 1180, inference loss is 0.46, accuracy is 0.781250, time 279.730 samples/sec\n",
      "Training: epoch 3, total_step 1190, inference loss is 0.31, accuracy is 0.890625, time 360.229 samples/sec\n",
      "Training: epoch 3, total_step 1200, inference loss is 0.44, accuracy is 0.851562, time 233.418 samples/sec\n",
      "Training: epoch 3, total_step 1210, inference loss is 0.35, accuracy is 0.898438, time 224.307 samples/sec\n",
      "Training: epoch 3, total_step 1220, inference loss is 0.32, accuracy is 0.890625, time 290.391 samples/sec\n",
      "Training: epoch 3, total_step 1230, inference loss is 0.29, accuracy is 0.890625, time 239.213 samples/sec\n",
      "Training: epoch 3, total_step 1240, inference loss is 0.38, accuracy is 0.875000, time 232.385 samples/sec\n",
      "Training: epoch 3, total_step 1250, inference loss is 0.55, accuracy is 0.796875, time 283.442 samples/sec\n",
      "Training: epoch 3, total_step 1260, inference loss is 0.40, accuracy is 0.851562, time 222.033 samples/sec\n",
      "Training: epoch 3, total_step 1270, inference loss is 0.38, accuracy is 0.843750, time 222.525 samples/sec\n",
      "Training: epoch 3, total_step 1280, inference loss is 0.37, accuracy is 0.882812, time 262.717 samples/sec\n",
      "Training: epoch 3, total_step 1290, inference loss is 0.35, accuracy is 0.914062, time 253.580 samples/sec\n",
      "Training: epoch 3, total_step 1300, inference loss is 0.34, accuracy is 0.875000, time 216.319 samples/sec\n",
      "Training: epoch 3, total_step 1310, inference loss is 0.41, accuracy is 0.867188, time 242.928 samples/sec\n",
      "Training: epoch 3, total_step 1320, inference loss is 0.29, accuracy is 0.882812, time 249.001 samples/sec\n",
      "Training: epoch 3, total_step 1330, inference loss is 0.31, accuracy is 0.921875, time 200.251 samples/sec\n",
      "Training: epoch 3, total_step 1340, inference loss is 0.32, accuracy is 0.898438, time 225.067 samples/sec\n",
      "Training: epoch 3, total_step 1350, inference loss is 0.31, accuracy is 0.914062, time 185.260 samples/sec\n",
      "Training: epoch 3, total_step 1360, inference loss is 0.38, accuracy is 0.859375, time 206.940 samples/sec\n",
      "Training: epoch 3, total_step 1370, inference loss is 0.32, accuracy is 0.882812, time 215.807 samples/sec\n",
      "Training: epoch 3, total_step 1380, inference loss is 0.40, accuracy is 0.859375, time 211.940 samples/sec\n",
      "Training: epoch 3, total_step 1390, inference loss is 0.41, accuracy is 0.851562, time 238.646 samples/sec\n",
      "Training: epoch 3, total_step 1400, inference loss is 0.39, accuracy is 0.828125, time 256.530 samples/sec\n",
      "Training: epoch 3, total_step 1410, inference loss is 0.39, accuracy is 0.851562, time 229.459 samples/sec\n",
      "Training: epoch 3, total_step 1420, inference loss is 0.50, accuracy is 0.835938, time 253.251 samples/sec\n",
      "Training: epoch 3, total_step 1430, inference loss is 0.34, accuracy is 0.882812, time 225.502 samples/sec\n",
      "Training: epoch 3, total_step 1440, inference loss is 0.34, accuracy is 0.906250, time 211.119 samples/sec\n",
      "Training: epoch 3, total_step 1450, inference loss is 0.26, accuracy is 0.882812, time 276.252 samples/sec\n",
      "Training: epoch 3, total_step 1460, inference loss is 0.34, accuracy is 0.898438, time 200.501 samples/sec\n",
      "Training: epoch 3, total_step 1470, inference loss is 0.38, accuracy is 0.851562, time 281.085 samples/sec\n",
      "Training: epoch 3, total_step 1480, inference loss is 0.36, accuracy is 0.859375, time 191.809 samples/sec\n",
      "Training: epoch 3, total_step 1490, inference loss is 0.31, accuracy is 0.875000, time 208.731 samples/sec\n",
      "Training: epoch 3, total_step 1500, inference loss is 0.39, accuracy is 0.875000, time 239.069 samples/sec\n",
      "Training: epoch 3, total_step 1510, inference loss is 0.32, accuracy is 0.882812, time 175.762 samples/sec\n",
      "Training: epoch 3, total_step 1520, inference loss is 0.30, accuracy is 0.890625, time 157.489 samples/sec\n",
      "Training: epoch 3, total_step 1530, inference loss is 0.31, accuracy is 0.898438, time 166.957 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 3, total_step 1540, inference loss is 0.32, accuracy is 0.898438, time 194.449 samples/sec\n",
      "Training: epoch 3, total_step 1550, inference loss is 0.35, accuracy is 0.875000, time 217.363 samples/sec\n",
      "End of epoch 3\n",
      "$$$$$$$$ Validation: epoch 3, accuracy is 0.925181\n",
      "Training: epoch 4, total_step 1560, inference loss is 0.29, accuracy is 0.914062, time 346.445 samples/sec\n",
      "Training: epoch 4, total_step 1570, inference loss is 0.33, accuracy is 0.875000, time 251.839 samples/sec\n",
      "Training: epoch 4, total_step 1580, inference loss is 0.34, accuracy is 0.859375, time 299.351 samples/sec\n",
      "Training: epoch 4, total_step 1590, inference loss is 0.30, accuracy is 0.882812, time 294.970 samples/sec\n",
      "Training: epoch 4, total_step 1600, inference loss is 0.43, accuracy is 0.867188, time 293.573 samples/sec\n",
      "Training: epoch 4, total_step 1610, inference loss is 0.38, accuracy is 0.882812, time 210.752 samples/sec\n",
      "Training: epoch 4, total_step 1620, inference loss is 0.30, accuracy is 0.898438, time 141.902 samples/sec\n",
      "Training: epoch 4, total_step 1630, inference loss is 0.24, accuracy is 0.929688, time 179.134 samples/sec\n",
      "Training: epoch 4, total_step 1640, inference loss is 0.48, accuracy is 0.804688, time 202.034 samples/sec\n",
      "Training: epoch 4, total_step 1650, inference loss is 0.31, accuracy is 0.890625, time 268.490 samples/sec\n",
      "Training: epoch 4, total_step 1660, inference loss is 0.33, accuracy is 0.882812, time 157.588 samples/sec\n",
      "Training: epoch 4, total_step 1670, inference loss is 0.34, accuracy is 0.859375, time 201.895 samples/sec\n",
      "Training: epoch 4, total_step 1680, inference loss is 0.30, accuracy is 0.898438, time 163.645 samples/sec\n",
      "Training: epoch 4, total_step 1690, inference loss is 0.30, accuracy is 0.875000, time 267.004 samples/sec\n",
      "Training: epoch 4, total_step 1700, inference loss is 0.34, accuracy is 0.914062, time 222.412 samples/sec\n",
      "Training: epoch 4, total_step 1710, inference loss is 0.22, accuracy is 0.929688, time 285.669 samples/sec\n",
      "Training: epoch 4, total_step 1720, inference loss is 0.42, accuracy is 0.875000, time 265.868 samples/sec\n",
      "Training: epoch 4, total_step 1730, inference loss is 0.27, accuracy is 0.906250, time 253.863 samples/sec\n",
      "Training: epoch 4, total_step 1740, inference loss is 0.27, accuracy is 0.898438, time 220.713 samples/sec\n",
      "Training: epoch 4, total_step 1750, inference loss is 0.35, accuracy is 0.890625, time 211.599 samples/sec\n",
      "Training: epoch 4, total_step 1760, inference loss is 0.29, accuracy is 0.906250, time 263.247 samples/sec\n",
      "Training: epoch 4, total_step 1770, inference loss is 0.33, accuracy is 0.875000, time 264.304 samples/sec\n",
      "Training: epoch 4, total_step 1780, inference loss is 0.26, accuracy is 0.906250, time 226.695 samples/sec\n",
      "Training: epoch 4, total_step 1790, inference loss is 0.30, accuracy is 0.890625, time 219.964 samples/sec\n",
      "Training: epoch 4, total_step 1800, inference loss is 0.36, accuracy is 0.875000, time 232.752 samples/sec\n",
      "Training: epoch 4, total_step 1810, inference loss is 0.29, accuracy is 0.898438, time 213.062 samples/sec\n",
      "Training: epoch 4, total_step 1820, inference loss is 0.39, accuracy is 0.835938, time 252.438 samples/sec\n",
      "Training: epoch 4, total_step 1830, inference loss is 0.30, accuracy is 0.890625, time 219.819 samples/sec\n",
      "Training: epoch 4, total_step 1840, inference loss is 0.25, accuracy is 0.921875, time 188.311 samples/sec\n",
      "Training: epoch 4, total_step 1850, inference loss is 0.33, accuracy is 0.867188, time 263.121 samples/sec\n",
      "Training: epoch 4, total_step 1860, inference loss is 0.20, accuracy is 0.929688, time 188.233 samples/sec\n",
      "Training: epoch 4, total_step 1870, inference loss is 0.41, accuracy is 0.851562, time 232.314 samples/sec\n",
      "Training: epoch 4, total_step 1880, inference loss is 0.28, accuracy is 0.914062, time 173.112 samples/sec\n",
      "Training: epoch 4, total_step 1890, inference loss is 0.27, accuracy is 0.921875, time 152.049 samples/sec\n",
      "Training: epoch 4, total_step 1900, inference loss is 0.22, accuracy is 0.906250, time 162.334 samples/sec\n",
      "Training: epoch 4, total_step 1910, inference loss is 0.32, accuracy is 0.882812, time 175.432 samples/sec\n",
      "Training: epoch 4, total_step 1920, inference loss is 0.26, accuracy is 0.898438, time 216.557 samples/sec\n",
      "Training: epoch 4, total_step 1930, inference loss is 0.26, accuracy is 0.914062, time 173.549 samples/sec\n",
      "Training: epoch 4, total_step 1940, inference loss is 0.36, accuracy is 0.851562, time 166.010 samples/sec\n",
      "End of epoch 4\n",
      "$$$$$$$$ Validation: epoch 4, accuracy is 0.938651\n",
      "Training: epoch 5, total_step 1950, inference loss is 0.27, accuracy is 0.906250, time 285.315 samples/sec\n",
      "Training: epoch 5, total_step 1960, inference loss is 0.42, accuracy is 0.835938, time 257.446 samples/sec\n",
      "Training: epoch 5, total_step 1970, inference loss is 0.29, accuracy is 0.906250, time 263.459 samples/sec\n",
      "Training: epoch 5, total_step 1980, inference loss is 0.37, accuracy is 0.882812, time 247.983 samples/sec\n",
      "Training: epoch 5, total_step 1990, inference loss is 0.33, accuracy is 0.859375, time 295.033 samples/sec\n",
      "Training: epoch 5, total_step 2000, inference loss is 0.22, accuracy is 0.945312, time 211.577 samples/sec\n",
      "Training: epoch 5, total_step 2010, inference loss is 0.27, accuracy is 0.906250, time 256.285 samples/sec\n",
      "Training: epoch 5, total_step 2020, inference loss is 0.26, accuracy is 0.914062, time 313.096 samples/sec\n",
      "Training: epoch 5, total_step 2030, inference loss is 0.36, accuracy is 0.867188, time 241.274 samples/sec\n",
      "Training: epoch 5, total_step 2040, inference loss is 0.27, accuracy is 0.914062, time 187.601 samples/sec\n",
      "Training: epoch 5, total_step 2050, inference loss is 0.29, accuracy is 0.898438, time 232.838 samples/sec\n",
      "Training: epoch 5, total_step 2060, inference loss is 0.38, accuracy is 0.859375, time 223.818 samples/sec\n",
      "Training: epoch 5, total_step 2070, inference loss is 0.41, accuracy is 0.882812, time 222.558 samples/sec\n",
      "Training: epoch 5, total_step 2080, inference loss is 0.30, accuracy is 0.851562, time 230.347 samples/sec\n",
      "Training: epoch 5, total_step 2090, inference loss is 0.32, accuracy is 0.882812, time 255.882 samples/sec\n",
      "Training: epoch 5, total_step 2100, inference loss is 0.52, accuracy is 0.781250, time 213.106 samples/sec\n",
      "Training: epoch 5, total_step 2110, inference loss is 0.25, accuracy is 0.929688, time 230.187 samples/sec\n",
      "Training: epoch 5, total_step 2120, inference loss is 0.39, accuracy is 0.859375, time 258.948 samples/sec\n",
      "Training: epoch 5, total_step 2130, inference loss is 0.35, accuracy is 0.859375, time 238.011 samples/sec\n",
      "Training: epoch 5, total_step 2140, inference loss is 0.39, accuracy is 0.859375, time 267.185 samples/sec\n",
      "Training: epoch 5, total_step 2150, inference loss is 0.19, accuracy is 0.937500, time 250.362 samples/sec\n",
      "Training: epoch 5, total_step 2160, inference loss is 0.31, accuracy is 0.875000, time 261.511 samples/sec\n",
      "Training: epoch 5, total_step 2170, inference loss is 0.32, accuracy is 0.843750, time 285.680 samples/sec\n",
      "Training: epoch 5, total_step 2180, inference loss is 0.32, accuracy is 0.859375, time 163.575 samples/sec\n",
      "Training: epoch 5, total_step 2190, inference loss is 0.26, accuracy is 0.906250, time 233.840 samples/sec\n",
      "Training: epoch 5, total_step 2200, inference loss is 0.35, accuracy is 0.890625, time 212.331 samples/sec\n",
      "Training: epoch 5, total_step 2210, inference loss is 0.31, accuracy is 0.882812, time 244.494 samples/sec\n",
      "Training: epoch 5, total_step 2220, inference loss is 0.35, accuracy is 0.890625, time 285.074 samples/sec\n",
      "Training: epoch 5, total_step 2230, inference loss is 0.37, accuracy is 0.859375, time 259.241 samples/sec\n",
      "Training: epoch 5, total_step 2240, inference loss is 0.25, accuracy is 0.921875, time 259.731 samples/sec\n",
      "Training: epoch 5, total_step 2250, inference loss is 0.36, accuracy is 0.882812, time 204.795 samples/sec\n",
      "Training: epoch 5, total_step 2260, inference loss is 0.33, accuracy is 0.859375, time 259.515 samples/sec\n",
      "Training: epoch 5, total_step 2270, inference loss is 0.28, accuracy is 0.898438, time 183.409 samples/sec\n",
      "Training: epoch 5, total_step 2280, inference loss is 0.29, accuracy is 0.882812, time 229.628 samples/sec\n",
      "Training: epoch 5, total_step 2290, inference loss is 0.28, accuracy is 0.906250, time 241.786 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 5, total_step 2300, inference loss is 0.37, accuracy is 0.859375, time 284.798 samples/sec\n",
      "Training: epoch 5, total_step 2310, inference loss is 0.33, accuracy is 0.875000, time 241.331 samples/sec\n",
      "Training: epoch 5, total_step 2320, inference loss is 0.29, accuracy is 0.914062, time 231.893 samples/sec\n",
      "Training: epoch 5, total_step 2330, inference loss is 0.39, accuracy is 0.843750, time 240.502 samples/sec\n",
      "End of epoch 5\n",
      "$$$$$$$$ Validation: epoch 5, accuracy is 0.934341\n",
      "Training: epoch 6, total_step 2340, inference loss is 0.50, accuracy is 0.812500, time 237.709 samples/sec\n",
      "Training: epoch 6, total_step 2350, inference loss is 0.37, accuracy is 0.898438, time 243.676 samples/sec\n",
      "Training: epoch 6, total_step 2360, inference loss is 0.32, accuracy is 0.890625, time 214.316 samples/sec\n",
      "Training: epoch 6, total_step 2370, inference loss is 0.24, accuracy is 0.914062, time 293.164 samples/sec\n",
      "Training: epoch 6, total_step 2380, inference loss is 0.30, accuracy is 0.882812, time 273.484 samples/sec\n",
      "Training: epoch 6, total_step 2390, inference loss is 0.27, accuracy is 0.898438, time 261.566 samples/sec\n",
      "Training: epoch 6, total_step 2400, inference loss is 0.31, accuracy is 0.875000, time 280.082 samples/sec\n",
      "Training: epoch 6, total_step 2410, inference loss is 0.27, accuracy is 0.906250, time 276.436 samples/sec\n",
      "Training: epoch 6, total_step 2420, inference loss is 0.24, accuracy is 0.921875, time 259.730 samples/sec\n",
      "Training: epoch 6, total_step 2430, inference loss is 0.24, accuracy is 0.906250, time 321.221 samples/sec\n",
      "Training: epoch 6, total_step 2440, inference loss is 0.23, accuracy is 0.921875, time 216.281 samples/sec\n",
      "Training: epoch 6, total_step 2450, inference loss is 0.22, accuracy is 0.890625, time 226.845 samples/sec\n",
      "Training: epoch 6, total_step 2460, inference loss is 0.42, accuracy is 0.851562, time 218.741 samples/sec\n",
      "Training: epoch 6, total_step 2470, inference loss is 0.42, accuracy is 0.898438, time 300.666 samples/sec\n",
      "Training: epoch 6, total_step 2480, inference loss is 0.38, accuracy is 0.875000, time 307.769 samples/sec\n",
      "Training: epoch 6, total_step 2490, inference loss is 0.29, accuracy is 0.914062, time 234.244 samples/sec\n",
      "Training: epoch 6, total_step 2500, inference loss is 0.22, accuracy is 0.914062, time 352.660 samples/sec\n",
      "Training: epoch 6, total_step 2510, inference loss is 0.29, accuracy is 0.898438, time 296.601 samples/sec\n",
      "Training: epoch 6, total_step 2520, inference loss is 0.17, accuracy is 0.945312, time 238.060 samples/sec\n",
      "Training: epoch 6, total_step 2530, inference loss is 0.19, accuracy is 0.921875, time 212.703 samples/sec\n",
      "Training: epoch 6, total_step 2540, inference loss is 0.27, accuracy is 0.929688, time 236.868 samples/sec\n",
      "Training: epoch 6, total_step 2550, inference loss is 0.25, accuracy is 0.914062, time 248.920 samples/sec\n",
      "Training: epoch 6, total_step 2560, inference loss is 0.44, accuracy is 0.851562, time 246.562 samples/sec\n",
      "Training: epoch 6, total_step 2570, inference loss is 0.31, accuracy is 0.875000, time 281.158 samples/sec\n",
      "Training: epoch 6, total_step 2580, inference loss is 0.37, accuracy is 0.890625, time 245.007 samples/sec\n",
      "Training: epoch 6, total_step 2590, inference loss is 0.26, accuracy is 0.929688, time 205.100 samples/sec\n",
      "Training: epoch 6, total_step 2600, inference loss is 0.36, accuracy is 0.851562, time 238.460 samples/sec\n",
      "Training: epoch 6, total_step 2610, inference loss is 0.28, accuracy is 0.882812, time 255.864 samples/sec\n",
      "Training: epoch 6, total_step 2620, inference loss is 0.29, accuracy is 0.875000, time 255.838 samples/sec\n",
      "Training: epoch 6, total_step 2630, inference loss is 0.25, accuracy is 0.914062, time 231.676 samples/sec\n",
      "Training: epoch 6, total_step 2640, inference loss is 0.26, accuracy is 0.882812, time 250.949 samples/sec\n",
      "Training: epoch 6, total_step 2650, inference loss is 0.24, accuracy is 0.929688, time 258.062 samples/sec\n",
      "Training: epoch 6, total_step 2660, inference loss is 0.26, accuracy is 0.937500, time 273.169 samples/sec\n",
      "Training: epoch 6, total_step 2670, inference loss is 0.25, accuracy is 0.882812, time 249.645 samples/sec\n",
      "Training: epoch 6, total_step 2680, inference loss is 0.33, accuracy is 0.851562, time 215.463 samples/sec\n",
      "Training: epoch 6, total_step 2690, inference loss is 0.26, accuracy is 0.906250, time 250.212 samples/sec\n",
      "Training: epoch 6, total_step 2700, inference loss is 0.35, accuracy is 0.898438, time 255.255 samples/sec\n",
      "Training: epoch 6, total_step 2710, inference loss is 0.31, accuracy is 0.898438, time 236.646 samples/sec\n",
      "Training: epoch 6, total_step 2720, inference loss is 0.37, accuracy is 0.882812, time 244.683 samples/sec\n",
      "End of epoch 6\n",
      "$$$$$$$$ Validation: epoch 6, accuracy is 0.932724\n",
      "Training: epoch 7, total_step 2730, inference loss is 0.36, accuracy is 0.867188, time 310.749 samples/sec\n",
      "Training: epoch 7, total_step 2740, inference loss is 0.34, accuracy is 0.867188, time 297.672 samples/sec\n",
      "Training: epoch 7, total_step 2750, inference loss is 0.32, accuracy is 0.890625, time 263.216 samples/sec\n",
      "Training: epoch 7, total_step 2760, inference loss is 0.21, accuracy is 0.929688, time 206.152 samples/sec\n",
      "Training: epoch 7, total_step 2770, inference loss is 0.35, accuracy is 0.890625, time 268.542 samples/sec\n",
      "Training: epoch 7, total_step 2780, inference loss is 0.33, accuracy is 0.890625, time 265.440 samples/sec\n",
      "Training: epoch 7, total_step 2790, inference loss is 0.21, accuracy is 0.914062, time 277.901 samples/sec\n",
      "Training: epoch 7, total_step 2800, inference loss is 0.36, accuracy is 0.875000, time 273.261 samples/sec\n",
      "Training: epoch 7, total_step 2810, inference loss is 0.26, accuracy is 0.929688, time 211.068 samples/sec\n",
      "Training: epoch 7, total_step 2820, inference loss is 0.49, accuracy is 0.828125, time 234.763 samples/sec\n",
      "Training: epoch 7, total_step 2830, inference loss is 0.31, accuracy is 0.890625, time 252.708 samples/sec\n",
      "Training: epoch 7, total_step 2840, inference loss is 0.27, accuracy is 0.890625, time 268.621 samples/sec\n",
      "Training: epoch 7, total_step 2850, inference loss is 0.21, accuracy is 0.929688, time 252.118 samples/sec\n",
      "Training: epoch 7, total_step 2860, inference loss is 0.33, accuracy is 0.882812, time 222.562 samples/sec\n",
      "Training: epoch 7, total_step 2870, inference loss is 0.37, accuracy is 0.867188, time 224.662 samples/sec\n",
      "Training: epoch 7, total_step 2880, inference loss is 0.36, accuracy is 0.851562, time 216.811 samples/sec\n",
      "Training: epoch 7, total_step 2890, inference loss is 0.23, accuracy is 0.921875, time 270.999 samples/sec\n",
      "Training: epoch 7, total_step 2900, inference loss is 0.37, accuracy is 0.882812, time 237.756 samples/sec\n",
      "Training: epoch 7, total_step 2910, inference loss is 0.23, accuracy is 0.937500, time 277.492 samples/sec\n",
      "Training: epoch 7, total_step 2920, inference loss is 0.33, accuracy is 0.882812, time 201.149 samples/sec\n",
      "Training: epoch 7, total_step 2930, inference loss is 0.26, accuracy is 0.921875, time 222.552 samples/sec\n",
      "Training: epoch 7, total_step 2940, inference loss is 0.27, accuracy is 0.898438, time 185.621 samples/sec\n",
      "Training: epoch 7, total_step 2950, inference loss is 0.24, accuracy is 0.890625, time 265.603 samples/sec\n",
      "Training: epoch 7, total_step 2960, inference loss is 0.19, accuracy is 0.929688, time 198.782 samples/sec\n",
      "Training: epoch 7, total_step 2970, inference loss is 0.28, accuracy is 0.906250, time 220.531 samples/sec\n",
      "Training: epoch 7, total_step 2980, inference loss is 0.27, accuracy is 0.906250, time 250.307 samples/sec\n",
      "Training: epoch 7, total_step 2990, inference loss is 0.30, accuracy is 0.890625, time 237.325 samples/sec\n",
      "Training: epoch 7, total_step 3000, inference loss is 0.29, accuracy is 0.921875, time 252.208 samples/sec\n",
      "Training: epoch 7, total_step 3010, inference loss is 0.23, accuracy is 0.898438, time 299.703 samples/sec\n",
      "Training: epoch 7, total_step 3020, inference loss is 0.25, accuracy is 0.929688, time 209.271 samples/sec\n",
      "Training: epoch 7, total_step 3030, inference loss is 0.28, accuracy is 0.882812, time 269.850 samples/sec\n",
      "Training: epoch 7, total_step 3040, inference loss is 0.27, accuracy is 0.914062, time 207.254 samples/sec\n",
      "Training: epoch 7, total_step 3050, inference loss is 0.32, accuracy is 0.921875, time 221.483 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 7, total_step 3060, inference loss is 0.26, accuracy is 0.937500, time 219.041 samples/sec\n",
      "Training: epoch 7, total_step 3070, inference loss is 0.30, accuracy is 0.898438, time 198.677 samples/sec\n",
      "Training: epoch 7, total_step 3080, inference loss is 0.23, accuracy is 0.929688, time 207.978 samples/sec\n",
      "Training: epoch 7, total_step 3090, inference loss is 0.31, accuracy is 0.875000, time 203.673 samples/sec\n",
      "Training: epoch 7, total_step 3100, inference loss is 0.31, accuracy is 0.898438, time 207.799 samples/sec\n",
      "Training: epoch 7, total_step 3110, inference loss is 0.30, accuracy is 0.882812, time 218.457 samples/sec\n",
      "End of epoch 7\n",
      "$$$$$$$$ Validation: epoch 7, accuracy is 0.936337\n",
      "Training: epoch 8, total_step 3120, inference loss is 0.26, accuracy is 0.906250, time 275.795 samples/sec\n",
      "Training: epoch 8, total_step 3130, inference loss is 0.16, accuracy is 0.953125, time 267.882 samples/sec\n",
      "Training: epoch 8, total_step 3140, inference loss is 0.26, accuracy is 0.890625, time 255.311 samples/sec\n",
      "Training: epoch 8, total_step 3150, inference loss is 0.22, accuracy is 0.914062, time 300.592 samples/sec\n",
      "Training: epoch 8, total_step 3160, inference loss is 0.20, accuracy is 0.937500, time 238.434 samples/sec\n",
      "Training: epoch 8, total_step 3170, inference loss is 0.41, accuracy is 0.828125, time 217.745 samples/sec\n",
      "Training: epoch 8, total_step 3180, inference loss is 0.32, accuracy is 0.898438, time 202.651 samples/sec\n",
      "Training: epoch 8, total_step 3190, inference loss is 0.20, accuracy is 0.937500, time 227.649 samples/sec\n",
      "Training: epoch 8, total_step 3200, inference loss is 0.31, accuracy is 0.898438, time 299.061 samples/sec\n",
      "Training: epoch 8, total_step 3210, inference loss is 0.34, accuracy is 0.882812, time 255.909 samples/sec\n",
      "Training: epoch 8, total_step 3220, inference loss is 0.29, accuracy is 0.906250, time 274.434 samples/sec\n",
      "Training: epoch 8, total_step 3230, inference loss is 0.19, accuracy is 0.945312, time 284.464 samples/sec\n",
      "Training: epoch 8, total_step 3240, inference loss is 0.31, accuracy is 0.898438, time 295.367 samples/sec\n",
      "Training: epoch 8, total_step 3250, inference loss is 0.33, accuracy is 0.921875, time 254.510 samples/sec\n",
      "Training: epoch 8, total_step 3260, inference loss is 0.32, accuracy is 0.906250, time 218.215 samples/sec\n",
      "Training: epoch 8, total_step 3270, inference loss is 0.30, accuracy is 0.898438, time 198.302 samples/sec\n",
      "Training: epoch 8, total_step 3280, inference loss is 0.31, accuracy is 0.882812, time 208.055 samples/sec\n",
      "Training: epoch 8, total_step 3290, inference loss is 0.35, accuracy is 0.890625, time 289.331 samples/sec\n",
      "Training: epoch 8, total_step 3300, inference loss is 0.27, accuracy is 0.890625, time 179.569 samples/sec\n",
      "Training: epoch 8, total_step 3310, inference loss is 0.35, accuracy is 0.882812, time 212.173 samples/sec\n",
      "Training: epoch 8, total_step 3320, inference loss is 0.27, accuracy is 0.890625, time 200.422 samples/sec\n",
      "Training: epoch 8, total_step 3330, inference loss is 0.27, accuracy is 0.898438, time 250.391 samples/sec\n",
      "Training: epoch 8, total_step 3340, inference loss is 0.31, accuracy is 0.914062, time 218.635 samples/sec\n",
      "Training: epoch 8, total_step 3350, inference loss is 0.20, accuracy is 0.945312, time 206.603 samples/sec\n",
      "Training: epoch 8, total_step 3360, inference loss is 0.20, accuracy is 0.921875, time 233.942 samples/sec\n",
      "Training: epoch 8, total_step 3370, inference loss is 0.19, accuracy is 0.968750, time 249.598 samples/sec\n",
      "Training: epoch 8, total_step 3380, inference loss is 0.35, accuracy is 0.890625, time 199.523 samples/sec\n",
      "Training: epoch 8, total_step 3390, inference loss is 0.25, accuracy is 0.898438, time 220.991 samples/sec\n",
      "Training: epoch 8, total_step 3400, inference loss is 0.25, accuracy is 0.914062, time 297.632 samples/sec\n",
      "Training: epoch 8, total_step 3410, inference loss is 0.29, accuracy is 0.914062, time 221.401 samples/sec\n",
      "Training: epoch 8, total_step 3420, inference loss is 0.29, accuracy is 0.882812, time 217.514 samples/sec\n",
      "Training: epoch 8, total_step 3430, inference loss is 0.23, accuracy is 0.882812, time 191.890 samples/sec\n",
      "Training: epoch 8, total_step 3440, inference loss is 0.25, accuracy is 0.906250, time 230.866 samples/sec\n",
      "Training: epoch 8, total_step 3450, inference loss is 0.28, accuracy is 0.898438, time 181.028 samples/sec\n",
      "Training: epoch 8, total_step 3460, inference loss is 0.34, accuracy is 0.867188, time 208.303 samples/sec\n",
      "Training: epoch 8, total_step 3470, inference loss is 0.16, accuracy is 0.945312, time 226.266 samples/sec\n",
      "Training: epoch 8, total_step 3480, inference loss is 0.26, accuracy is 0.898438, time 246.506 samples/sec\n",
      "Training: epoch 8, total_step 3490, inference loss is 0.23, accuracy is 0.914062, time 277.090 samples/sec\n",
      "Training: epoch 8, total_step 3500, inference loss is 0.30, accuracy is 0.901639, time 325.964 samples/sec\n",
      "End of epoch 8\n",
      "$$$$$$$$ Validation: epoch 8, accuracy is 0.948031\n",
      "Training: epoch 9, total_step 3510, inference loss is 0.25, accuracy is 0.921875, time 237.443 samples/sec\n",
      "Training: epoch 9, total_step 3520, inference loss is 0.35, accuracy is 0.859375, time 306.579 samples/sec\n",
      "Training: epoch 9, total_step 3530, inference loss is 0.27, accuracy is 0.906250, time 234.138 samples/sec\n",
      "Training: epoch 9, total_step 3540, inference loss is 0.16, accuracy is 0.960938, time 346.638 samples/sec\n",
      "Training: epoch 9, total_step 3550, inference loss is 0.32, accuracy is 0.882812, time 240.300 samples/sec\n",
      "Training: epoch 9, total_step 3560, inference loss is 0.19, accuracy is 0.953125, time 276.340 samples/sec\n",
      "Training: epoch 9, total_step 3570, inference loss is 0.31, accuracy is 0.890625, time 236.438 samples/sec\n",
      "Training: epoch 9, total_step 3580, inference loss is 0.26, accuracy is 0.921875, time 335.630 samples/sec\n",
      "Training: epoch 9, total_step 3590, inference loss is 0.19, accuracy is 0.937500, time 269.600 samples/sec\n",
      "Training: epoch 9, total_step 3600, inference loss is 0.18, accuracy is 0.953125, time 273.075 samples/sec\n",
      "Training: epoch 9, total_step 3610, inference loss is 0.27, accuracy is 0.921875, time 264.925 samples/sec\n",
      "Training: epoch 9, total_step 3620, inference loss is 0.30, accuracy is 0.890625, time 294.045 samples/sec\n",
      "Training: epoch 9, total_step 3630, inference loss is 0.31, accuracy is 0.906250, time 211.365 samples/sec\n",
      "Training: epoch 9, total_step 3640, inference loss is 0.28, accuracy is 0.898438, time 236.870 samples/sec\n",
      "Training: epoch 9, total_step 3650, inference loss is 0.40, accuracy is 0.867188, time 239.106 samples/sec\n",
      "Training: epoch 9, total_step 3660, inference loss is 0.32, accuracy is 0.906250, time 224.210 samples/sec\n",
      "Training: epoch 9, total_step 3670, inference loss is 0.26, accuracy is 0.898438, time 196.204 samples/sec\n",
      "Training: epoch 9, total_step 3680, inference loss is 0.31, accuracy is 0.882812, time 221.172 samples/sec\n",
      "Training: epoch 9, total_step 3690, inference loss is 0.35, accuracy is 0.867188, time 229.838 samples/sec\n",
      "Training: epoch 9, total_step 3700, inference loss is 0.30, accuracy is 0.906250, time 214.392 samples/sec\n",
      "Training: epoch 9, total_step 3710, inference loss is 0.28, accuracy is 0.890625, time 228.918 samples/sec\n",
      "Training: epoch 9, total_step 3720, inference loss is 0.23, accuracy is 0.906250, time 233.101 samples/sec\n",
      "Training: epoch 9, total_step 3730, inference loss is 0.31, accuracy is 0.898438, time 244.609 samples/sec\n",
      "Training: epoch 9, total_step 3740, inference loss is 0.27, accuracy is 0.875000, time 257.963 samples/sec\n",
      "Training: epoch 9, total_step 3750, inference loss is 0.42, accuracy is 0.882812, time 246.182 samples/sec\n",
      "Training: epoch 9, total_step 3760, inference loss is 0.32, accuracy is 0.898438, time 186.433 samples/sec\n",
      "Training: epoch 9, total_step 3770, inference loss is 0.17, accuracy is 0.937500, time 287.487 samples/sec\n",
      "Training: epoch 9, total_step 3780, inference loss is 0.31, accuracy is 0.875000, time 234.082 samples/sec\n",
      "Training: epoch 9, total_step 3790, inference loss is 0.18, accuracy is 0.929688, time 268.796 samples/sec\n",
      "Training: epoch 9, total_step 3800, inference loss is 0.21, accuracy is 0.929688, time 270.223 samples/sec\n",
      "Training: epoch 9, total_step 3810, inference loss is 0.31, accuracy is 0.882812, time 265.900 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 9, total_step 3820, inference loss is 0.16, accuracy is 0.937500, time 217.177 samples/sec\n",
      "Training: epoch 9, total_step 3830, inference loss is 0.42, accuracy is 0.843750, time 203.652 samples/sec\n",
      "Training: epoch 9, total_step 3840, inference loss is 0.26, accuracy is 0.898438, time 205.952 samples/sec\n",
      "Training: epoch 9, total_step 3850, inference loss is 0.25, accuracy is 0.914062, time 234.104 samples/sec\n",
      "Training: epoch 9, total_step 3860, inference loss is 0.20, accuracy is 0.921875, time 248.619 samples/sec\n",
      "Training: epoch 9, total_step 3870, inference loss is 0.19, accuracy is 0.921875, time 236.323 samples/sec\n",
      "Training: epoch 9, total_step 3880, inference loss is 0.27, accuracy is 0.890625, time 250.320 samples/sec\n",
      "End of epoch 9\n",
      "$$$$$$$$ Validation: epoch 9, accuracy is 0.948753\n"
     ]
    }
   ],
   "source": [
    "img_verify_flag = False # set to false during actual training\n",
    "count = 0\n",
    "validation_result = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(config.NUM_EPOCH):\n",
    "    \n",
    "    sess.run(iterator_train.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            images_train, labels_train = sess.run(next_element_train)\n",
    "            #images_train, images_show, labels_train = sess.run(next_element_train) # for dataset debug\n",
    "            if img_verify_flag: # for dataset debug\n",
    "                img = Image.fromarray(images_show[0,...], 'RGB')\n",
    "                img.show()\n",
    "                exit(1)\n",
    "            feed_dict = {features_ph: images_train, labels_ph: labels_train}            \n",
    "            embTrain, logitTrain, inferenceLossTrain, accTrain, _, _ = \\\n",
    "                sess.run([embeddings, logit, inference_loss, acc, train_op, inc_op],\n",
    "                          feed_dict=feed_dict,\n",
    "                          options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n",
    "            end = time.time()\n",
    "            pre_sec = config.BATCH_SIZE/(end - start)\n",
    "            \n",
    "            if count > 0 and count % config.Update_Interval == 0:\n",
    "                # logging\n",
    "                print('Training: epoch %d, total_step %d, inference loss is %.2f, '\n",
    "                      'accuracy is %.6f, time %.3f samples/sec' %\n",
    "                          (i, count, inferenceLossTrain, accTrain,pre_sec))\n",
    "                log_file.write('Training: epoch %d, total_step %d, inference_loss %.2f, '\n",
    "                               'accuracy %.6f, time %.3f samples/sec' %\n",
    "                               (i, count, inferenceLossTrain, accTrain,pre_sec) + '\\n')\n",
    "                log_file.flush()\n",
    "                \n",
    "                # save summary\n",
    "                summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n",
    "                summary.add_summary(summary_op_val, count)\n",
    "                \n",
    "                #print(embTrain)\n",
    "                #print(logitTrain)\n",
    "                #print(inferenceLossTrain)\n",
    "                #print(labels_train)\n",
    "                #print(images_train)\n",
    "            count += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of epoch %d\" % i)\n",
    "            break\n",
    "    \n",
    "    # save check points\n",
    "    ckpt_filename = config.PREFIX+'_{:d}'.format(i) + '.ckpt'\n",
    "    ckpt_filename = os.path.join(checkpointsPath, ckpt_filename)\n",
    "    saver.save(sess, ckpt_filename)\n",
    "    \n",
    "            \n",
    "    # do validation\n",
    "    accVal = []\n",
    "    predVal = []\n",
    "    labelVal = np.array([])\n",
    "    sess.run(iterator_val.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            images_val, labels_val = sess.run(next_element_val)\n",
    "            feed_dict = {features_val_ph: images_val, labels_val_ph: labels_val}\n",
    "            acc_tmp, pred_tmp = \\\n",
    "                sess.run([acc_val, pred_val],\n",
    "                          feed_dict=feed_dict,\n",
    "                          options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))                        \n",
    "            accVal += [acc_tmp]\n",
    "            #print(accVal)\n",
    "            \n",
    "            if type(predVal) == type([]):\n",
    "                predVal = pred_tmp\n",
    "            else:\n",
    "                predVal = np.append(predVal, pred_tmp, axis=0)\n",
    "            labelVal = np.append(labelVal, labels_val)\n",
    "            \n",
    "            #print(predVal)\n",
    "            #print(labelVal)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break    \n",
    "    accVal = np.mean(accVal)\n",
    "    print('$$$$$$$$ Validation: epoch %d, accuracy is %.6f' % (i, accVal))\n",
    "    log_file.write('Validation: epoch %d, accuracy %.6f' % (i, accVal) + '\\n')\n",
    "    log_file.flush()\n",
    "    \n",
    "    # save validation results\n",
    "    validation_result += [{'label': labelVal, 'pred': predVal}]\n",
    "    with open(checkpointsPath+'/'+config.PREFIX+'_val_result.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_result, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {features_ph: feature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "emb_out=sess.run([embedding],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = tf.cast(0.1, dtype=tf.bool)\n",
    "aaa=sess.run(aa)\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(net),net[0].shape,net[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = SpoofDenseNet.build(net, [128,64,16], True, tf.nn.relu, config.Regularizer['CoALBP_GREY'], config.Initializer, scope=\"firstTry1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[1].shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.fromarray(images[0,...], 'RGB')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['image/height'][0])\n",
    "print(features['image/width'][0])\n",
    "print(features['image/class/text'][0])\n",
    "print(features['image/class/label'][0])\n",
    "print(len(features['image/encoded'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.image.decode_jpeg(features['image/encoded'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sess.run(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(name='global_step', initial_value=0, trainable=False)\n",
    "inc_op = tf.assign_add(global_step, 1, name='increment_global_step')\n",
    "lr = tf.train.piecewise_constant(global_step, boundaries=config.Opt_lr_steps, values=config.Opt_lr, name='lr_schedule')\n",
    "# define the optimize method\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=config.Opt_momentum)\n",
    "# get train op\n",
    "#grads = opt.compute_gradients(inference_loss)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):        \n",
    "    #train_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    train_op = opt.minimize(inference_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=True)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    y = tf.identity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "x_1 = [[-10], [0], [10]]\n",
    "x_2 = [[-10]]\n",
    "for _ in range(1000):\n",
    "    y_1 = sess.run(y, feed_dict={x: x_1, is_training: False})\n",
    "y_2 = sess.run(y, feed_dict={x: x_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
